{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DL_CW1.ipynb","provenance":[],"authorship_tag":"ABX9TyPG4CK0h/Tuo8I9NrN0z+Hj"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"5p8M_1dzDpGC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614032908395,"user_tz":0,"elapsed":1209,"user":{"displayName":"abhinav gyani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiU5rB7xwgv8Whhs10EEH-bIaZ0sJhfm9669VIC=s64","userId":"11401909896438186779"}},"outputId":"b09e1fca-8276-4068-9527-bed56b56225e"},"source":["import numpy as np\r\n","import torch\r\n","import torch.nn as nn\r\n","import torch.nn.functional as F\r\n","from scipy import misc\r\n","from scipy import ndimage\r\n","from skimage import metrics as mt\r\n","import imageio\r\n","\r\n","\r\n","def imread(path, is_grayscale=True):\r\n","    \"\"\"\r\n","    Read image from the giving path.\r\n","    Default value is gray-scale, and image is read by YCbCr format as the paper.\r\n","    \"\"\"\r\n","    if is_grayscale:\r\n","        return imageio.imread(path, as_gray=True, pilmode='YCbCr').astype(np.float32)\r\n","    else:\r\n","        return imageio.imread(path, pilmode='YCbCr').astype(np.float32)\r\n","\r\n","def modcrop(image, scale=3):\r\n","    \"\"\"\r\n","    To scale down and up the original image, first thing to do is to have no remainder while scaling operation.\r\n","\r\n","    We need to find modulo of height (and width) and scale factor.\r\n","    Then, subtract the modulo from height (and width) of original image size.\r\n","    There would be no remainder even after scaling operation.\r\n","    \"\"\"\r\n","    if len(image.shape) == 3:\r\n","        h, w, _ = image.shape\r\n","        h = h - np.mod(h, scale)\r\n","        w = w - np.mod(w, scale)\r\n","        image = image[0:h, 0:w, :]\r\n","    else:\r\n","        h, w = image.shape\r\n","        h = h - np.mod(h, scale)\r\n","        w = w - np.mod(w, scale)\r\n","        image = image[0:h, 0:w]\r\n","    return image\r\n","\r\n","def preprocess(path, scale=3):\r\n","    \"\"\"\r\n","    Preprocess single image file\r\n","      (1) Read original image as YCbCr format (and grayscale as default)\r\n","      (2) Normalize\r\n","      (3) Apply image file with bicubic interpolation\r\n","    Args:\r\n","      path: file path of desired file\r\n","      input_: image applied bicubic interpolation (low-resolution)\r\n","      label_: image with original resolution (high-resolution)\r\n","    \"\"\"\r\n","    image = imread(path, is_grayscale=True)\r\n","    label_ = modcrop(image, scale)\r\n","\r\n","    # Must be normalized\r\n","    label_ = label_ / 255.\r\n","\r\n","    input_ = ndimage.interpolation.zoom(label_, (1. / scale), prefilter=False)\r\n","    input_ = ndimage.interpolation.zoom(input_, (scale / 1.), prefilter=False)\r\n","\r\n","    return input_, label_\r\n","\r\n","\"\"\"Define the model weights and biases \r\n","\"\"\"\r\n","## ------ Add your code here: set the weight of three conv layers\r\n","# replace 'None' with your hyper parameter numbers\r\n","# conv1 layer with biases: 64 filters with size 9 x 9\r\n","# conv2 layer with biases and relu: 32 filters with size 1 x 1\r\n","# conv3 layer with biases and NO relu: 1 filter with size 5 x 5\r\n","\r\n","class SRCNN(nn.Module):\r\n","    def __init__(self):\r\n","        super(SRCNN, self).__init__()\r\n","        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=9, padding=4)\r\n","        self.conv2 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=1, padding=0)\r\n","        self.conv3 = nn.Conv2d(in_channels=32, out_channels=1, kernel_size=5, padding=2)\r\n","        \r\n","    def forward(self, x):\r\n","        out = F.relu(self.conv1(x))\r\n","        out = F.relu(self.conv2(out))\r\n","        out = self.conv3(out)\r\n","        return out\r\n","\r\n","\"\"\"Load the pre-trained model file\r\n","\"\"\"\r\n","model = SRCNN()\r\n","model.load_state_dict(torch.load('./model.pth'))\r\n","model.eval()\r\n","\r\n","\"\"\"Read the test image\r\n","\"\"\"\r\n","LR_image, HR_image = preprocess('./butterfly_GT.bmp')\r\n","# transform the input to 4-D tensor\r\n","input_ = np.expand_dims(np.expand_dims(LR_image, axis=0), axis=0)\r\n","input_ = torch.from_numpy(input_)\r\n","\r\n","\"\"\"Run the model and get the SR image\r\n","\"\"\"\r\n","with torch.no_grad():\r\n","    output_ = model(input_)\r\n","\r\n","imageio.imsave('LR_image.bmp', LR_image)\r\n","imageio.imsave('HR_image.bmp', HR_image)\r\n","\r\n","PSNR_BIC = mt.peak_signal_noise_ratio(LR_image,HR_image)\r\n","print(PSNR_BIC)\r\n","\r\n","SR_image = output_.numpy()\r\n","SR_image = np.reshape(SR_image, (255,255))\r\n","imageio.imsave('SR_image.bmp', SR_image)\r\n","\r\n","PSNR_SRCNN = mt.peak_signal_noise_ratio(HR_image,SR_image)\r\n","print(PSNR_SRCNN)\r\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"],"name":"stderr"},{"output_type":"stream","text":["20.497630181368823\n","22.922696428588342\n"],"name":"stdout"}]}]}