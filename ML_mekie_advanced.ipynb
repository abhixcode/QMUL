{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ML_mekie_advanced.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"FeAqPx6Q3dVU","executionInfo":{"status":"ok","timestamp":1618176799444,"user_tz":-60,"elapsed":4518,"user":{"displayName":"abhinav gyani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiU5rB7xwgv8Whhs10EEH-bIaZ0sJhfm9669VIC=s64","userId":"11401909896438186779"}}},"source":["# python library imports\n","import librosa\n","import librosa.display\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import os, sys, re, pickle, glob\n","from PIL import Image\n","import pathlib\n","import csv \n","# sklearn.model_selection import train_test_split\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","import keras\n","from keras import layers\n","from keras.models import Sequential\n","import warnings\n","warnings.filterwarnings('ignore')\n","import IPython.display as ipd"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Iqb8xWH6Xjd"},"source":["**Visualization of the Audio Files:**\n","In this section, we have read and plotted the MileEnd audio segments for better understanding of the samples."]},{"cell_type":"code","metadata":{"id":"gLADMwtA5F5c"},"source":["cmap = plt.get_cmap('inferno')\n","plt.figure(figsize=(8,8))\n","files = glob.glob('/content/drive/MyDrive/QMUL/ML/Project/Data/MLEnd/training/*/*.wav')\n","\n","intonations = 'neutral bored excited question'.split()\n","for filename in files[:5]:        \n","    y, sr = librosa.load(filename, mono=True, duration=5)\n","    plt.specgram(y, NFFT=2048, Fs=2, Fc=0, noverlap=128, cmap=cmap, sides='default', mode='default', scale='dB');\n","    plt.axis('off');\n","    plt.savefig(f'/content/drive/MyDrive/QMUL/ML/Project/Data/MLEnd/img_data/{os.path.basename(filename).replace(\".wav\", \"\")}.png')\n","    plt.show()\n","\n","    # plt.figure(figsize=(14, 5))\n","    # librosa.display.waveplot(y, sr=sr)\n","\n","    plt.clf()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WDtc5G732Tvc"},"source":["**Data Preprocessing:**\n","Arranging all the feature header in this section."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yu4avI7P9iy_","executionInfo":{"status":"ok","timestamp":1618176817402,"user_tz":-60,"elapsed":544,"user":{"displayName":"abhinav gyani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiU5rB7xwgv8Whhs10EEH-bIaZ0sJhfm9669VIC=s64","userId":"11401909896438186779"}},"outputId":"76f84b35-071b-4363-cd40-cd5ac4d8696d"},"source":["header = 'filename chroma_stft rmse spectral_centroid spectral_bandwidth rolloff zero_crossing_rate'\n","for i in range(1, 21):\n","    header += f' mfcc{i}'\n","header += ' label'\n","header = header.split()\n","\n","print(header)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["['filename', 'chroma_stft', 'rmse', 'spectral_centroid', 'spectral_bandwidth', 'rolloff', 'zero_crossing_rate', 'mfcc1', 'mfcc2', 'mfcc3', 'mfcc4', 'mfcc5', 'mfcc6', 'mfcc7', 'mfcc8', 'mfcc9', 'mfcc10', 'mfcc11', 'mfcc12', 'mfcc13', 'mfcc14', 'mfcc15', 'mfcc16', 'mfcc17', 'mfcc18', 'mfcc19', 'mfcc20', 'label']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Bvm1xvCI2mOA"},"source":["**Labels Processing:**\n","Loading all the labels into a variable to process and feed into our model."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AmN7h05SENkU","executionInfo":{"status":"ok","timestamp":1618176858462,"user_tz":-60,"elapsed":566,"user":{"displayName":"abhinav gyani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiU5rB7xwgv8Whhs10EEH-bIaZ0sJhfm9669VIC=s64","userId":"11401909896438186779"}},"outputId":"383a85b8-34e7-4243-a9bf-ac52b666089f"},"source":["labels = pd.read_csv('trainingMLEnd.csv')\n","labels['digit_label']"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0         4\n","1         2\n","2        70\n","3         2\n","4         4\n","         ..\n","19995    90\n","19996    10\n","19997    90\n","19998    19\n","19999    20\n","Name: digit_label, Length: 20000, dtype: int64"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1AzdReCPL2fO","executionInfo":{"status":"ok","timestamp":1618176864572,"user_tz":-60,"elapsed":539,"user":{"displayName":"abhinav gyani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiU5rB7xwgv8Whhs10EEH-bIaZ0sJhfm9669VIC=s64","userId":"11401909896438186779"}},"outputId":"ee2f650a-88cb-453b-a611-e442f8472f53"},"source":["labels['File ID']"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0        0000000.wav\n","1        0000001.wav\n","2        0000002.wav\n","3        0000003.wav\n","4        0000004.wav\n","            ...     \n","19995    0019995.wav\n","19996    0019996.wav\n","19997    0019997.wav\n","19998    0019998.wav\n","19999    0019999.wav\n","Name: File ID, Length: 20000, dtype: object"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"5ec-40X620jw"},"source":["**Feature Extraction:**\n","Extracting features from first 5000 audio files and saving them into a CSV file which in turn will be used as the input of our model."]},{"cell_type":"code","metadata":{"id":"J_bsdW16Faz4"},"source":["new_dataset_path = '/content/drive/MyDrive/QMUL/ML/Project/Data/MLEnd/new_dataset.csv'\n","file = open(new_dataset_path, 'w', newline='')\n","with file:\n","    writer = csv.writer(file)\n","    writer.writerow(header)\n","\n","digit_idx = 0\n","for filename in labels['File ID'][:5000]:\n","# for filename in files[:50]:    \n","    filename = f'/content/drive/MyDrive/QMUL/ML/Project/Data/MLEnd/training/Training/{filename}'\n","    y, sr = librosa.load(filename, mono=True, duration=30)\n","    digit_label = labels['digit_label'][digit_idx]\n","    digit_idx += 1\n","    # print(filename, '=>', digit_label)\n","    if (digit_label) > 9:\n","      continue\n","\n","    # rmse = librosa.feature.rmse(y=y)\n","    rmse = librosa.feature.rms(y=y)[0]\n","    chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n","    spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n","    spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n","    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n","    zcr = librosa.feature.zero_crossing_rate(y)\n","    mfcc = librosa.feature.mfcc(y=y, sr=sr)\n","    filename2 = os.path.basename(filename)\n","    to_append = f'{filename2} {np.mean(chroma_stft)} {np.mean(rmse)} {np.mean(spec_cent)} {np.mean(spec_bw)} {np.mean(rolloff)} {np.mean(zcr)}'    \n","    for e in mfcc:\n","        to_append += f' {np.mean(e)}'\n","    to_append += f' {digit_label}'\n","    file = open(new_dataset_path, 'a', newline='')\n","    with file:\n","        writer = csv.writer(file)\n","        writer.writerow(to_append.split())     "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vkpdSC6Q3ELi"},"source":["**Visualizing Feature Data:**\n","Here we are visualing the features that were saved into a CSV file."]},{"cell_type":"code","metadata":{"id":"l4RQqo_giGLn"},"source":["data = pd.read_csv(new_dataset_path)\n","data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FKil3Ynk3QXC"},"source":["**Feature Selection and Preparing Training & Test Data:**\n","In this section we remove all unncessary data from the feature CSV, transform them and split them to achieve Training & Test datasets."]},{"cell_type":"code","metadata":{"id":"w1nop21BI4EU"},"source":["data.head() # Dropping unneccesary columns\n","data = data.drop(['filename'],axis=1) # Encoding the Labels\n","digit_list = data.iloc[:, -1]\n","print(digit_list)\n","encoder = LabelEncoder()\n","y = encoder.fit_transform(digit_list) # Scaling the Feature columns\n","scaler = StandardScaler()\n","X = scaler.fit_transform(np.array(data.iloc[:, :-1], dtype = float)) # Dividing data into training and Testing set\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q0rdXZ1a359O"},"source":["**Visualizing the Features**"]},{"cell_type":"code","metadata":{"id":"LthRsKYOW7Ip"},"source":["data.iloc[:, :-1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NLJ7NrARXHaJ"},"source":["np.array(data.iloc[:, :-1], dtype = float)[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wD2bc7ISVThr"},"source":["X_train"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c0wC-VCv4ERs"},"source":["**Checking the label data**"]},{"cell_type":"code","metadata":{"id":"f5ei0eUxVhVS"},"source":["y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6E-tFyL34J7Y"},"source":["**Building the Model:**\n","Here we have built our model. This is an ANN model with activatation parameters in several layers."]},{"cell_type":"code","metadata":{"id":"3IH0P8xwJWrY"},"source":["model = Sequential()\n","model.add(layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)))\n","model.add(layers.Dense(128, activation='relu'))\n","model.add(layers.Dense(64, activation='relu'))\n","model.add(layers.Dense(10, activation='softmax'))\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-0w5pf834fgP"},"source":["**Model Summary**"]},{"cell_type":"code","metadata":{"id":"exSlHEhrVvUT"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p-pbH3TG4kF4"},"source":["**Model Fitting and Classification:**\n","In this phase, we have fitted the model with training data for a number of 150 epochs with batch size of 128."]},{"cell_type":"code","metadata":{"id":"l2uekVdVJcju"},"source":["classifier = model.fit(X_train,\n","                    y_train,\n","                    epochs=150,\n","                    batch_size=128)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vJdOUcGJ455b"},"source":["**Checking Accuracy Data**"]},{"cell_type":"code","metadata":{"id":"JgfNI0RrP5UX"},"source":["train_accuracy=model.evaluate(X_train,y_train,verbose=0)\n","print(train_accuracy[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RR9tdIGpOOMJ"},"source":["test_accuracy=model.evaluate(X_test,y_test,verbose=0)\n","print(test_accuracy[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bCJ8UNuGRnYu"},"source":["X_test[1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KQ-W73UDso63"},"source":["**Visualization with Confusion Matrix**"]},{"cell_type":"code","metadata":{"id":"6Xxg3LSHsu6P"},"source":["from sklearn import neighbors\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","\n","yt_p = model.predict(X_train)\n","print(y_train)\n","print(yt_p)\n","train_confusion_matrix = confusion_matrix(y_true=y_train, y_pred=yt_p)\n","print('Training confusion matrix:\\n {}\\n'.format(train_confusion_matrix))\n","\n","# confusion matrix\n","y_pred = yt_p\n","cf_matrix = confusion_matrix(y_train, y_pred)\n","sns.heatmap(cf_matrix, annot=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xXN1cFQ74-hJ"},"source":["**Testing the Model:**\n","We chose a random audio file from the MileEnd audio dataset and verified whether our model could predit the correct number."]},{"cell_type":"code","metadata":{"id":"sVQLtg14Rpec"},"source":["filename = '0008039.wav' # digit file\n","filename = f'/content/drive/MyDrive/QMUL/ML/Project/Data/MLEnd/training/Training/{filename}'\n","y, sr = librosa.load(filename, mono=True, duration=30)\n","\n","rmse = librosa.feature.rms(y=y)[0]\n","chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n","spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n","spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n","rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n","zcr = librosa.feature.zero_crossing_rate(y)\n","mfcc = librosa.feature.mfcc(y=y, sr=sr)\n","to_append = f'{np.mean(chroma_stft)} {np.mean(rmse)} {np.mean(spec_cent)} {np.mean(spec_bw)} {np.mean(rolloff)} {np.mean(zcr)}'    \n","for e in mfcc:\n","    to_append += f' {np.mean(e)}'\n","\n","data = np.array(to_append.split(), dtype = float)\n","print(data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1WzZGaET5W4o"},"source":["**Reshaping the Test Input and Pulling the Prediction Values**"]},{"cell_type":"code","metadata":{"id":"Fypv1XlyYFs-"},"source":["data_reshaped = data.reshape(1,-1)\n","predicted_label=model.predict_classes(data_reshaped)\n","print(predicted_label)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dkTUkUJqY-HQ"},"source":["prediction_class = encoder.inverse_transform(predicted_label) \n","prediction_class"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-qQXlf6Mro7T"},"source":["**Conclusion:**\n","\n","In this solution, we used the MLEnd dataset, we splitted it, we extracted features and trained the model. Our model was an Artificial Neural Network which gave us significant results. Finally, we proved by our test that the model can predict the correct numeral from any given MLEnd audio segment."]}]}